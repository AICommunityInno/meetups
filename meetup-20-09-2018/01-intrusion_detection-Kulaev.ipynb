{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">INFORMATION SECURITY MARKET ESTIMATION</h2> \n",
    "\n",
    "![title](imgs/cs_market_estimation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">ANOTHER TOOL TO DETECT MALWARE</h2> \n",
    "\n",
    "If you have infected computers, the management server can communicate with them through domains that are automatically generated by the hackers' software to send commands and updates.\n",
    "\n",
    "Explanation can be read at https://en.wikipedia.org/wiki/Domain_generation_algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">GENERATE DOMAINS BY YOURSELF</h2> \n",
    "\n",
    "![title](imgs/hacker.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hlxclhpmcajwaquf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example from wikipedia\n",
    "\n",
    "def generate_domain(year, month, day):\n",
    "    \"\"\"Generates a domain name for the given date.\"\"\"\n",
    "    domain = \"\"\n",
    "\n",
    "    for i in range(16):\n",
    "        year = ((year ^ 8 * year) >> 11) ^ ((year & 0xFFFFFFF0) << 17)\n",
    "        month = ((month ^ 4 * month) >> 25) ^ 16 * (month & 0xFFFFFFF8)\n",
    "        day = ((day ^ (day << 13)) >> 19) ^ ((day & 0xFFFFFFFE) << 12)\n",
    "        domain += chr(((year ^ month ^ day) % 25) + 97)\n",
    "\n",
    "    return domain\n",
    "\n",
    "generate_domain(2018, 9, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">DGA DOMAINS CLASSIFICATION</h2> \n",
    "<br /><center>Recognition of automatically generated domains can be considered as a binary classification problem.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do we have data for analysis?\n",
    "\n",
    "Yes, in short, if we log network traffic in some way, then we can rip the logs of requests from computers on the network, whether corporate, home, public or some other, from there we can get the domain, analyze it, and issue our verdict for further decision.\n",
    "\n",
    "#### Do we have labeled data to learn and test the classifier?\n",
    "\n",
    "In fact, yes. We need to know that security engineers around the world are trying to get the algorithms that hackers use, in order to prevent attacks, there are two types of obtaining such information.\n",
    "The first method is widely used in business - this is the method of reverse engineering.\n",
    "The second way is to grab the software of intruders.\n",
    "Both work fine.\n",
    "\n",
    "See https://github.com/andrewaeva/DGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Let's start</h2>\n",
    "\n",
    "The very first thing we are to do when we deal with such type of problems is to get raw dataset and prepare it for the next step. We know that our dataset is clean enough, that is why we should miss some steps which are common probably for the most of ML tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     1000000\n",
      "False     801667\n",
      "Name: is_legitime, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>is_legitime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>839127</th>\n",
       "      <td>goxxxfuck.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658213</th>\n",
       "      <td>xuatnhapcanh.com.vn</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559779</th>\n",
       "      <td>workerscompensation.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523342</th>\n",
       "      <td>spnlnnvurq.org</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741030</th>\n",
       "      <td>verraes.net</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369438</th>\n",
       "      <td>crossroadstrading.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969917</th>\n",
       "      <td>lcsnw.org</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967331</th>\n",
       "      <td>liyandigital.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774831</th>\n",
       "      <td>insaneproductivity.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292860</th>\n",
       "      <td>pahabeow.ru</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         domain  is_legitime\n",
       "839127            goxxxfuck.com         True\n",
       "658213      xuatnhapcanh.com.vn         True\n",
       "559779  workerscompensation.com         True\n",
       "523342           spnlnnvurq.org        False\n",
       "741030              verraes.net         True\n",
       "369438    crossroadstrading.com         True\n",
       "969917                lcsnw.org         True\n",
       "967331         liyandigital.com         True\n",
       "774831   insaneproductivity.com         True\n",
       "292860              pahabeow.ru        False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "all_legit = pd.read_csv('data/domains/all_legit.txt', delimiter=' ', names=['domain', 'label']).dropna()\n",
    "all_dga = pd.read_csv('data/domains/all_dga.txt', delimiter=' ', names=['domain', 'label']).dropna()\n",
    "raw_data = pd.concat([all_legit, all_dga])\n",
    "\n",
    "# prepare column of labels to binary classification\n",
    "data = raw_data.copy()\n",
    "data['label'] = data['label'].apply(\n",
    "    lambda x:\n",
    "    True if x == 0\n",
    "    else False\n",
    ")\n",
    "data.rename(columns={'label':'is_legitime'}, inplace=True)\n",
    "\n",
    "# take a view\n",
    "print(data['is_legitime'].value_counts())\n",
    "data.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have dataset, it is quite balanced (0.555 positive : 0.445 negative).\n",
    "\n",
    "Our next step is to extract features. For some reason we will not extract features like first letter or get dummy from the letters included.\n",
    "We will concentrate on features which probably will impact on the predictions the most, our experience in the related field and dozens of scientific papers, articles and reports from companies and independent researchers in the field of cybersecurity will tell us what to look for first, actually this is the way, usually data scientists solve issues or build baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!better to do some explanations and provide intuitions!!!!!!!!!!\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import tldextract\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def max_subword_len(word):\n",
    "\n",
    "    return max([len(subword) for subword in re.split(r'[\\d\\.\\-]+', word)])\n",
    "\n",
    "def specific_symbols_count(domain):\n",
    "\n",
    "    return sum((symbol.isdigit() | (symbol in['.','-','_'])) for symbol in domain)\n",
    "\n",
    "# count word entropy\n",
    "def count_entropy(word):\n",
    "\n",
    "    word_counter, word_len = Counter(word), float(len(word))\n",
    "\n",
    "    return -sum(\n",
    "        count / word_len * math.log(count / word_len, 2)\n",
    "        for count in word_counter.values()\n",
    "    )\n",
    "\n",
    "# extract info from frequency vocabulary\n",
    "def extract_vocab_n_counts(path):\n",
    "\n",
    "    words_lst=list()\n",
    "    with open(path) as file:\n",
    "        for line in file:\n",
    "            words_lst.append(line[:-1])\n",
    "    lang = pd.DataFrame(words_lst, columns = ['word'])\n",
    "    lang_vc = CountVectorizer(analyzer='char', ngram_range=(3, 5), min_df=1e-5, max_df=1.0)\n",
    "    lang_counts_matrix = lang_vc.fit_transform(lang['word'])\n",
    "    lang_counts = np.log10(lang_counts_matrix.sum(axis=0).getA1())\n",
    "\n",
    "    return lang_vc, lang_counts\n",
    "\n",
    "# prepare features from domains\n",
    "def extract_domain_based_features_to_df(blank_domains_df, vocab_data):\n",
    "\n",
    "    domains_df = blank_domains_df.copy()\n",
    "\n",
    "    domains_df['domain_name'] = domains_df['domain'].apply(lambda x: tldextract.extract(x).domain)\n",
    "    domains_df['domain_name_len'] = domains_df['domain_name'].apply(len)\n",
    "    domains_df['specific_symbols_count'] = domains_df['domain'].apply(specific_symbols_count)\n",
    "    domains_df['max_word_len'] = domains_df['domain'].apply(max_subword_len)\n",
    "    domains_df['percentage_of_specific'] = domains_df['specific_symbols_count'] / domains_df['domain_name_len']\n",
    "    domains_df['entropy'] = domains_df['domain'].apply(count_entropy)\n",
    "    en_vc, en_counts, tr_vc, tr_counts = vocab_data\n",
    "    domains_df['en_grams'] = en_counts * en_vc.transform(domains_df['domain_name']).T\n",
    "    domains_df['tr_grams'] = tr_counts * tr_vc.transform(domains_df['domain_name']).T\n",
    "\n",
    "    return domains_df\n",
    "\n",
    "en_vc, en_counts = extract_vocab_n_counts('data/vocabs/en.txt')\n",
    "tr_vc, tr_counts = extract_vocab_n_counts('data/vocabs/tr.txt')\n",
    "vocab_data = (en_vc, en_counts, tr_vc, tr_counts)\n",
    "\n",
    "prepared_data = extract_domain_based_features_to_df(data, vocab_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have prepared dataset, we have features vectors, we have labels for them.\n",
    "\n",
    "Now we are to split our data into training dataset and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !!!!!!!!!!better to visualise feature descriptions!!!!!!!!!!\n",
    "features_description = list(map(lambda x: (x[0], x[-1].describe()), prepared_data.groupby('is_legitime')))\n",
    "\n",
    "# split the data into labels and features\n",
    "X = prepared_data[\n",
    "    [\n",
    "        'domain_name_len', 'specific_symbols_count', 'max_word_len',\n",
    "        'percentage_of_specific', 'entropy', 'en_grams', 'tr_grams'\n",
    "    ]\n",
    "]\n",
    "y = prepared_data['is_legitime']\n",
    "\n",
    "# split our data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have data, we already know that we arediving into binary classification problem.\n",
    "\n",
    "So, what should we do?\n",
    "\n",
    "Know we are to:\n",
    "\n",
    "- select features\n",
    "- select the model\n",
    "- tune parameters\n",
    "- teach the classifier\n",
    "- test our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!better to go through the steps, not just showing the result!!!!!!!!!!\n",
    "\n",
    "from sklearn import metrics\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = XGBClassifier(\n",
    "    colsample_bytree=0.3,\n",
    "    min_child_weight=0.1,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    n_estimators=37,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "clf = model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "class_names = ['non_legitime', 'legitime']\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    import itertools\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
